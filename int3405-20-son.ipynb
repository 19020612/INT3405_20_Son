{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyvi\n!pip install tensorflow==1.14.0","metadata":{"execution":{"iopub.status.busy":"2022-12-13T08:06:34.931615Z","iopub.execute_input":"2022-12-13T08:06:34.931912Z","iopub.status.idle":"2022-12-13T08:06:58.307496Z","shell.execute_reply.started":"2022-12-13T08:06:34.931858Z","shell.execute_reply":"2022-12-13T08:06:58.306634Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyvi\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/27/27ffee2663f42430cf3434da963f04224fec157b90799fe9e92a3564c1a6/pyvi-0.1.1-py2.py3-none-any.whl (8.5MB)\n\u001b[K     |████████████████████████████████| 8.5MB 906kB/s eta 0:00:01\n\u001b[?25hCollecting sklearn-crfsuite (from pyvi)\n  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from pyvi) (0.21.2)\nRequirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.6/site-packages (from sklearn-crfsuite->pyvi) (4.32.1)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.6/site-packages/tabulate-0.8.3-py3.6.egg (from sklearn-crfsuite->pyvi) (0.8.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sklearn-crfsuite->pyvi) (1.12.0)\nCollecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->pyvi)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/b9/b6f48d74e10136ccfafbadcae751f3e81d143b40847d0f20728026783834/python-crfsuite-0.9.8.tar.gz (440kB)\n\u001b[K     |████████████████████████████████| 440kB 40.4MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->pyvi) (1.16.4)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->pyvi) (0.13.2)\nRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->pyvi) (1.2.1)\nBuilding wheels for collected packages: python-crfsuite\n  Building wheel for python-crfsuite (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/40/fa/ee/3afb15958ad26f3aef88d61c316b4d1af8a97660aa24e6e6d7\nSuccessfully built python-crfsuite\nInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\nSuccessfully installed python-crfsuite-0.9.8 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n#Ignoring the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re, string, unicodedata\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.pooling import GlobalMaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.layers import *\nfrom keras import backend\nfrom sklearn.metrics import f1_score, confusion_matrix\nimport tensorflow as tf\n\nfrom pyvi import ViTokenizer\nfrom pyvi import ViUtils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing the dataset\n***\n\nThe dataset: 'imdb_master.csv' is read and loaded as pandas dataframe.  \nLet's have a look at the data","metadata":{}},{"cell_type":"markdown","source":"# Importing the dataset\n","metadata":{}},{"cell_type":"code","source":"df2 = pd.read_csv('/kaggle/input/int3405-sentiment-analysis-problem/full_train.csv')\ndf2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train2 = pd.DataFrame({'input':df2['Comment'],'label':df2['Rating']})\ndata_train2.label = data_train2.label.apply(lambda x: 1. if x == 1 else 0.)\ndata_train2 = data_train2.dropna()\ndata_train2 = data_train2.reset_index(drop=True)\nX_train = list(data_train2['input'].values)\ny_train = list(data_train2['label'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Preprocessing\n***\nPreprocessing the text so as to have a better data for our model.  \nIt comprises of steps such as removing non-ASCII characters, removing HTML tags, converting to lower-case, lemmatizing.","metadata":{}},{"cell_type":"code","source":"#Function for Text Preprocessing\ndef clean_text(X,y):\n    idx = 0\n    y_train = []\n    processed = []\n    for text in X:\n        text = list(tf.keras.preprocessing.text.text_to_word_sequence(text))\n        text = \" \".join(text)\n        input_text_pre_no_accent = str(ViUtils.remove_accents(text).decode(\"utf-8\"))\n        input_text_pre_accent = ViTokenizer.tokenize(text)\n        input_text_pre_no_accent = ViTokenizer.tokenize(input_text_pre_no_accent)\n        processed.append(input_text_pre_accent)\n        processed.append(input_text_pre_no_accent)\n        y_train.append(y[idx])\n        y_train.append(y[idx])\n        idx += 1\n    return processed,y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocessing the Training Set and Test set","metadata":{}},{"cell_type":"code","source":"X_train_final,y_train = clean_text(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Attention Layer\n***\n\nThe basic concept of attention is that not all words contribute equally to the meaning of a sentence. Hence, their contribution must be weighted.  \nHow attention works is, it basically extracts words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector.","metadata":{}},{"cell_type":"code","source":"# Attention Layer\nclass AttentionWithContext(Layer):\n\n    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]\n\ndef dot_product(x, kernel):\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some Useful Variables  \n","metadata":{}},{"cell_type":"code","source":"#Tokenization and Padding\nvocab_size = 60000\nmaxlen = 300\nencode_dim = 20\nbatch_size = 32\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train_final)\ntokenized_word_list = tokenizer.texts_to_sequences(X_train_final)\nX_train_padded = pad_sequences(tokenized_word_list, maxlen = maxlen, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ModelCheckpoint\ncheckpoint = ModelCheckpoint('sentiment_classifier.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building the Model\n***\nThe model used comprises of CuDNNLSTM with Attention layer on top of it, followed by a dense layer and finally a dense layer with sigmoid activation function to get the sentiment or the class.  \nOptimiser used is ADAM","metadata":{}},{"cell_type":"code","source":"# Building the model\nmodel = Sequential()\nembed = Embedding(input_dim = vocab_size, output_dim = 32, input_length = X_train_padded.shape[1], dropout = 0.4) \nmodel.add(embed)\nmodel.add(Bidirectional(CuDNNLSTM(256, return_sequences = True)))\nmodel.add(Dropout(0.5))\nmodel.add(AttentionWithContext())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512))\nmodel.add(ReLU())\nmodel.add(Dense(256))\nmodel.add(ReLU())\nmodel.add(Dense(1, activation = 'sigmoid'))\nfrom keras.optimizers import adam\noptim =  adam(lr=1e-4)\nmodel.compile(loss = 'binary_crossentropy', \n              optimizer = optim, \n              metrics = ['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training\n***\nSplitting the Training set into Training set and Validation set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_final2, X_val, y_train_final2, y_val = train_test_split(X_train_padded, y_train, test_size = 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#class weight\nweight_for_0 = (1 / 0.27)#*((len(y))/2.0 )\nweight_for_1 = (1 / 0.73)#*((len(y))/2.0)\nclass_weight = {0: weight_for_0, 1: weight_for_1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint('sentiment_classifier.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fitting the model\nbatch_size= 256\n\nmodel.fit(X_train_final2, y_train_final2, \n          epochs = 35, batch_size = batch_size, verbose = 1,\n          validation_data = [X_val, y_val],\n          callbacks = [checkpoint],class_weight=class_weight)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing\n***\nConverting the test data into sequences of integers and padding them.  \nLoading the best model and calculating the accuracy","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/int3405-sentiment-analysis-problem/test.csv')\ndata_test = pd.DataFrame({'input':df['Comment'],'id':df[\"RevId\"]})\nX_test = data_test['input'].values\n\ndef clean_text_test(X):\n    processed = []\n    for text in X:\n        text = list(tf.keras.preprocessing.text.text_to_word_sequence(str(text)))\n        text = \" \".join(text)\n        input_text_pre_no_accent = str(ViUtils.remove_accents(text).decode(\"utf-8\"))\n        input_text_pre_accent = ViTokenizer.tokenize(text)\n        processed.append(input_text_pre_accent)\n    return processed\n\nX_test_final = clean_text_test(X_test)\n\ntokenized_word_list = tokenizer.texts_to_sequences(X_test_final)\nX_test_padded = pad_sequences(tokenized_word_list, maxlen = maxlen, padding='post')\ny_pred = model.predict(X_test_padded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\nmodelload = load_model('sentiment_classifier.h5', custom_objects = {\"AttentionWithContext\" : AttentionWithContext, \"backend\" : backend})\ny_pred = modelload.predict(X_test_padded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission['Rating'].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission = pd.DataFrame({'RevId': np.array(df[\"RevId\"]).reshape(5103), 'Rating': np.array(y_pred).reshape(5103)})\nmy_submission.to_csv('submitTrain.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}