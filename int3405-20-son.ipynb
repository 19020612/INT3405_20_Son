{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyvi\n!pip install tensorflow==1.14.0","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:37:57.958197Z","iopub.execute_input":"2022-12-13T13:37:57.958454Z","iopub.status.idle":"2022-12-13T13:38:25.335115Z","shell.execute_reply.started":"2022-12-13T13:37:57.958400Z","shell.execute_reply":"2022-12-13T13:38:25.334155Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyvi\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/27/27ffee2663f42430cf3434da963f04224fec157b90799fe9e92a3564c1a6/pyvi-0.1.1-py2.py3-none-any.whl (8.5MB)\n\u001b[K     |████████████████████████████████| 8.5MB 4.2MB/s eta 0:00:01\n\u001b[?25hCollecting sklearn-crfsuite (from pyvi)\n  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from pyvi) (0.21.2)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.6/site-packages/tabulate-0.8.3-py3.6.egg (from sklearn-crfsuite->pyvi) (0.8.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sklearn-crfsuite->pyvi) (1.12.0)\nRequirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.6/site-packages (from sklearn-crfsuite->pyvi) (4.32.1)\nCollecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->pyvi)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/b9/b6f48d74e10136ccfafbadcae751f3e81d143b40847d0f20728026783834/python-crfsuite-0.9.8.tar.gz (440kB)\n\u001b[K     |████████████████████████████████| 440kB 45.3MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->pyvi) (1.16.4)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->pyvi) (0.13.2)\nRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->pyvi) (1.2.1)\nBuilding wheels for collected packages: python-crfsuite\n  Building wheel for python-crfsuite (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/40/fa/ee/3afb15958ad26f3aef88d61c316b4d1af8a97660aa24e6e6d7\nSuccessfully built python-crfsuite\nInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\nSuccessfully installed python-crfsuite-0.9.8 pyvi-0.1.1 sklearn-crfsuite-0.3.6\nRequirement already satisfied: tensorflow==1.14.0 in /opt/conda/lib/python3.6/site-packages (1.14.0)\nRequirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (0.8.0)\nRequirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (0.2.2)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (0.7.1)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (1.1.0)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (1.1.0)\nRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (0.1.7)\nRequirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (1.14.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (1.12.0)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (1.22.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (0.33.4)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (1.11.2)\nRequirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (1.0.8)\nRequirement already satisfied: numpy<2.0,>=1.14.5 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (1.16.4)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (3.7.1)\nRequirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.14.0) (1.14.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (41.0.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (0.15.4)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.9.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n#Ignoring the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re, string, unicodedata\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.pooling import GlobalMaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.layers import *\nfrom keras import backend\nfrom sklearn.metrics import f1_score, confusion_matrix\nimport tensorflow as tf\n\nfrom pyvi import ViTokenizer\nfrom pyvi import ViUtils","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:25.339345Z","iopub.execute_input":"2022-12-13T13:38:25.339610Z","iopub.status.idle":"2022-12-13T13:38:28.226060Z","shell.execute_reply.started":"2022-12-13T13:38:25.339560Z","shell.execute_reply":"2022-12-13T13:38:28.225167Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Importing the dataset\n***\n\nThe dataset: 'imdb_master.csv' is read and loaded as pandas dataframe.  \nLet's have a look at the data","metadata":{}},{"cell_type":"markdown","source":"# Importing the dataset\n","metadata":{}},{"cell_type":"code","source":"df2 = pd.read_csv('/kaggle/input/int3405-sentiment-analysis-problem/full_train.csv')\ndf2.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:28.227519Z","iopub.execute_input":"2022-12-13T13:38:28.227818Z","iopub.status.idle":"2022-12-13T13:38:28.468702Z","shell.execute_reply.started":"2022-12-13T13:38:28.227769Z","shell.execute_reply":"2022-12-13T13:38:28.466330Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"  Unnamed: 0  ...   Rating\n0          0  ...      1.0\n1          1  ...      0.0\n2          2  ...      1.0\n3          3  ...      0.0\n4          4  ...      1.0\n\n[5 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>RevId</th>\n      <th>UserId</th>\n      <th>Comment</th>\n      <th>image_urls</th>\n      <th>Rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3839333</td>\n      <td>10106093.0</td>\n      <td>Xôi dẻo, đồ ăn đậm vị. Hộp xôi được lót lá trô...</td>\n      <td>['https://images.foody.vn/res/g97/966781/s800/...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2824877</td>\n      <td>786914.0</td>\n      <td>Gọi ship 1 xuất cari gà bánh naan và 3 miếng g...</td>\n      <td>['https://images.foody.vn/res/g69/688413/s800/...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>9816702</td>\n      <td>22467889.0</td>\n      <td>Thời tiết lạnh như này, cả nhà rủ nhau đến leg...</td>\n      <td>['https://images.foody.vn/res/g72/715078/s800/...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2684585</td>\n      <td>1889449.0</td>\n      <td>Em có đọc review thấy mng bảo trà sữa nướng đề...</td>\n      <td>['https://images.foody.vn/res/g90/895545/s800/...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2737987</td>\n      <td>8839942.0</td>\n      <td>Đồ ăn rất ngon, nhà hàng cũng rất đẹp, tất cả ...</td>\n      <td>['https://images.foody.vn/res/g4/30186/s800/fo...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_train2 = pd.DataFrame({'input':df2['Comment'],'label':df2['Rating']})\ndata_train2.label = data_train2.label.apply(lambda x: 1. if x == 1 else 0.)\ndata_train2 = data_train2.dropna()\ndata_train2 = data_train2.reset_index(drop=True)\nX_train = list(data_train2['input'].values)\ny_train = list(data_train2['label'].values)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:28.476505Z","iopub.execute_input":"2022-12-13T13:38:28.478969Z","iopub.status.idle":"2022-12-13T13:38:28.493877Z","shell.execute_reply.started":"2022-12-13T13:38:28.478925Z","shell.execute_reply":"2022-12-13T13:38:28.493155Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Text Preprocessing\n***\nPreprocessing the text so as to have a better data for our model.  \nIt comprises of steps such as removing non-ASCII characters, removing HTML tags, converting to lower-case, lemmatizing.","metadata":{}},{"cell_type":"code","source":"#Function for Text Preprocessing\ndef clean_text(X,y):\n    idx = 0\n    y_train = []\n    processed = []\n    for text in X:\n        text = list(tf.keras.preprocessing.text.text_to_word_sequence(text))\n        text = \" \".join(text)\n        input_text_pre_no_accent = str(ViUtils.remove_accents(text).decode(\"utf-8\"))\n        input_text_pre_accent = ViTokenizer.tokenize(text)\n        input_text_pre_no_accent = ViTokenizer.tokenize(input_text_pre_no_accent)\n        processed.append(input_text_pre_accent)\n        processed.append(input_text_pre_no_accent)\n        y_train.append(y[idx])\n        y_train.append(y[idx])\n        idx += 1\n    return processed,y_train","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:28.498205Z","iopub.execute_input":"2022-12-13T13:38:28.498655Z","iopub.status.idle":"2022-12-13T13:38:28.506064Z","shell.execute_reply.started":"2022-12-13T13:38:28.498462Z","shell.execute_reply":"2022-12-13T13:38:28.505240Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Preprocessing the Training Set and Test set","metadata":{}},{"cell_type":"code","source":"X_train_final,y_train = clean_text(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:28.507639Z","iopub.execute_input":"2022-12-13T13:38:28.508117Z","iopub.status.idle":"2022-12-13T13:38:53.451372Z","shell.execute_reply.started":"2022-12-13T13:38:28.508063Z","shell.execute_reply":"2022-12-13T13:38:53.450518Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Attention Layer\n***\n\nThe basic concept of attention is that not all words contribute equally to the meaning of a sentence. Hence, their contribution must be weighted.  \nHow attention works is, it basically extracts words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector.","metadata":{}},{"cell_type":"code","source":"# Attention Layer\nclass AttentionWithContext(Layer):\n\n    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]\n\ndef dot_product(x, kernel):\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:53.453019Z","iopub.execute_input":"2022-12-13T13:38:53.453527Z","iopub.status.idle":"2022-12-13T13:38:53.471891Z","shell.execute_reply.started":"2022-12-13T13:38:53.453468Z","shell.execute_reply":"2022-12-13T13:38:53.470902Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Some Useful Variables  \n","metadata":{}},{"cell_type":"code","source":"#Tokenization and Padding\nvocab_size = 60000\nmaxlen = 300\nencode_dim = 20\nbatch_size = 32\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train_final)\ntokenized_word_list = tokenizer.texts_to_sequences(X_train_final)\nX_train_padded = pad_sequences(tokenized_word_list, maxlen = maxlen, padding='post')","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:53.473217Z","iopub.execute_input":"2022-12-13T13:38:53.473692Z","iopub.status.idle":"2022-12-13T13:38:56.426690Z","shell.execute_reply.started":"2022-12-13T13:38:53.473511Z","shell.execute_reply":"2022-12-13T13:38:56.425826Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#ModelCheckpoint\ncheckpoint = ModelCheckpoint('sentiment_classifier.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:56.428212Z","iopub.execute_input":"2022-12-13T13:38:56.428522Z","iopub.status.idle":"2022-12-13T13:38:56.433203Z","shell.execute_reply.started":"2022-12-13T13:38:56.428471Z","shell.execute_reply":"2022-12-13T13:38:56.432120Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Building the Model\n***\nThe model used comprises of CuDNNLSTM with Attention layer on top of it, followed by a dense layer and finally a dense layer with sigmoid activation function to get the sentiment or the class.  \nOptimiser used is ADAM","metadata":{}},{"cell_type":"code","source":"# Building the model\nmodel = Sequential()\nembed = Embedding(input_dim = vocab_size, output_dim = 32, input_length = X_train_padded.shape[1], dropout = 0.4) \nmodel.add(embed)\nmodel.add(Bidirectional(CuDNNLSTM(256, return_sequences = True)))\nmodel.add(Dropout(0.5))\nmodel.add(AttentionWithContext())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512))\nmodel.add(ReLU())\nmodel.add(Dense(256))\nmodel.add(ReLU())\nmodel.add(Dense(1, activation = 'sigmoid'))\nfrom keras.optimizers import adam\noptim =  adam(lr=1e-4)\nmodel.compile(loss = 'binary_crossentropy', \n              optimizer = optim, \n              metrics = ['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:56.434582Z","iopub.execute_input":"2022-12-13T13:38:56.435164Z","iopub.status.idle":"2022-12-13T13:38:58.758514Z","shell.execute_reply.started":"2022-12-13T13:38:56.435106Z","shell.execute_reply":"2022-12-13T13:38:58.757799Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 300, 32)           1920000   \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 300, 512)          593920    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 300, 512)          0         \n_________________________________________________________________\nattention_with_context_1 (At (None, 512)               263168    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               262656    \n_________________________________________________________________\nre_lu_1 (ReLU)               (None, 512)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 256)               131328    \n_________________________________________________________________\nre_lu_2 (ReLU)               (None, 256)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 257       \n=================================================================\nTotal params: 3,171,329\nTrainable params: 3,171,329\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training\n***\nSplitting the Training set into Training set and Validation set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_final2, X_val, y_train_final2, y_val = train_test_split(X_train_padded, y_train, test_size = 0.1)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:58.759899Z","iopub.execute_input":"2022-12-13T13:38:58.760212Z","iopub.status.idle":"2022-12-13T13:38:58.786845Z","shell.execute_reply.started":"2022-12-13T13:38:58.760163Z","shell.execute_reply":"2022-12-13T13:38:58.786072Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#class weight\nweight_for_0 = (1 / 0.27)#*((len(y))/2.0 )\nweight_for_1 = (1 / 0.73)#*((len(y))/2.0)\nclass_weight = {0: weight_for_0, 1: weight_for_1}","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:58.788276Z","iopub.execute_input":"2022-12-13T13:38:58.788553Z","iopub.status.idle":"2022-12-13T13:38:58.793443Z","shell.execute_reply.started":"2022-12-13T13:38:58.788507Z","shell.execute_reply":"2022-12-13T13:38:58.792606Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint('sentiment_classifier.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:58.794809Z","iopub.execute_input":"2022-12-13T13:38:58.795397Z","iopub.status.idle":"2022-12-13T13:38:58.804140Z","shell.execute_reply.started":"2022-12-13T13:38:58.795345Z","shell.execute_reply":"2022-12-13T13:38:58.803312Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Fitting the model\nbatch_size= 256\n\nmodel.fit(X_train_final2, y_train_final2, \n          epochs = 35, batch_size = batch_size, verbose = 1,\n          validation_data = [X_val, y_val],\n          callbacks = [checkpoint],class_weight=class_weight)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:38:58.805715Z","iopub.execute_input":"2022-12-13T13:38:58.806275Z","iopub.status.idle":"2022-12-13T13:42:05.936227Z","shell.execute_reply.started":"2022-12-13T13:38:58.806081Z","shell.execute_reply":"2022-12-13T13:42:05.935170Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Train on 16327 samples, validate on 1815 samples\nEpoch 1/35\n16327/16327 [==============================] - 14s 869us/step - loss: 1.2754 - acc: 0.7878 - val_loss: 0.6173 - val_acc: 0.7846\n\nEpoch 00001: val_acc improved from -inf to 0.78457, saving model to sentiment_classifier.h5\nEpoch 2/35\n16327/16327 [==============================] - 8s 505us/step - loss: 1.2703 - acc: 0.7881 - val_loss: 0.6174 - val_acc: 0.7846\n\nEpoch 00002: val_acc did not improve from 0.78457\nEpoch 3/35\n16327/16327 [==============================] - 8s 504us/step - loss: 1.2688 - acc: 0.7881 - val_loss: 0.5936 - val_acc: 0.7846\n\nEpoch 00003: val_acc did not improve from 0.78457\nEpoch 4/35\n16327/16327 [==============================] - 8s 501us/step - loss: 1.1646 - acc: 0.7941 - val_loss: 0.5097 - val_acc: 0.7736\n\nEpoch 00004: val_acc did not improve from 0.78457\nEpoch 5/35\n16327/16327 [==============================] - 8s 502us/step - loss: 0.7101 - acc: 0.8732 - val_loss: 0.3382 - val_acc: 0.8727\n\nEpoch 00005: val_acc improved from 0.78457 to 0.87273, saving model to sentiment_classifier.h5\nEpoch 6/35\n16327/16327 [==============================] - 8s 505us/step - loss: 0.5925 - acc: 0.8978 - val_loss: 0.2841 - val_acc: 0.9085\n\nEpoch 00006: val_acc improved from 0.87273 to 0.90854, saving model to sentiment_classifier.h5\nEpoch 7/35\n16327/16327 [==============================] - 8s 504us/step - loss: 0.5458 - acc: 0.9122 - val_loss: 0.2848 - val_acc: 0.9019\n\nEpoch 00007: val_acc did not improve from 0.90854\nEpoch 8/35\n16327/16327 [==============================] - 8s 499us/step - loss: 0.5147 - acc: 0.9190 - val_loss: 0.2914 - val_acc: 0.8986\n\nEpoch 00008: val_acc did not improve from 0.90854\nEpoch 9/35\n16327/16327 [==============================] - 8s 501us/step - loss: 0.4866 - acc: 0.9253 - val_loss: 0.2591 - val_acc: 0.9107\n\nEpoch 00009: val_acc improved from 0.90854 to 0.91074, saving model to sentiment_classifier.h5\nEpoch 10/35\n16327/16327 [==============================] - 8s 507us/step - loss: 0.4559 - acc: 0.9313 - val_loss: 0.2705 - val_acc: 0.9069\n\nEpoch 00010: val_acc did not improve from 0.91074\nEpoch 11/35\n16327/16327 [==============================] - 8s 504us/step - loss: 0.4460 - acc: 0.9332 - val_loss: 0.2583 - val_acc: 0.9113\n\nEpoch 00011: val_acc improved from 0.91074 to 0.91129, saving model to sentiment_classifier.h5\nEpoch 12/35\n16327/16327 [==============================] - 8s 498us/step - loss: 0.4196 - acc: 0.9388 - val_loss: 0.2615 - val_acc: 0.9063\n\nEpoch 00012: val_acc did not improve from 0.91129\nEpoch 13/35\n16327/16327 [==============================] - 8s 501us/step - loss: 0.3946 - acc: 0.9430 - val_loss: 0.2696 - val_acc: 0.9041\n\nEpoch 00013: val_acc did not improve from 0.91129\nEpoch 14/35\n16327/16327 [==============================] - 8s 505us/step - loss: 0.3818 - acc: 0.9449 - val_loss: 0.2755 - val_acc: 0.9058\n\nEpoch 00014: val_acc did not improve from 0.91129\nEpoch 15/35\n16327/16327 [==============================] - 8s 502us/step - loss: 0.3530 - acc: 0.9524 - val_loss: 0.2828 - val_acc: 0.8992\n\nEpoch 00015: val_acc did not improve from 0.91129\nEpoch 16/35\n16327/16327 [==============================] - 8s 502us/step - loss: 0.3317 - acc: 0.9550 - val_loss: 0.2906 - val_acc: 0.9003\n\nEpoch 00016: val_acc did not improve from 0.91129\nEpoch 17/35\n16327/16327 [==============================] - 8s 507us/step - loss: 0.3131 - acc: 0.9574 - val_loss: 0.2851 - val_acc: 0.9003\n\nEpoch 00017: val_acc did not improve from 0.91129\nEpoch 18/35\n16327/16327 [==============================] - 8s 504us/step - loss: 0.2988 - acc: 0.9603 - val_loss: 0.3189 - val_acc: 0.8964\n\nEpoch 00018: val_acc did not improve from 0.91129\nEpoch 19/35\n16327/16327 [==============================] - 8s 499us/step - loss: 0.2842 - acc: 0.9620 - val_loss: 0.2907 - val_acc: 0.9025\n\nEpoch 00019: val_acc did not improve from 0.91129\nEpoch 20/35\n16327/16327 [==============================] - 8s 500us/step - loss: 0.2745 - acc: 0.9645 - val_loss: 0.3283 - val_acc: 0.8931\n\nEpoch 00020: val_acc did not improve from 0.91129\nEpoch 21/35\n16327/16327 [==============================] - 8s 506us/step - loss: 0.2530 - acc: 0.9668 - val_loss: 0.3205 - val_acc: 0.8970\n\nEpoch 00021: val_acc did not improve from 0.91129\nEpoch 22/35\n13056/16327 [======================>.......] - ETA: 1s - loss: 0.2480 - acc: 0.9687","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-7a07873f8e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           callbacks = [checkpoint],class_weight=class_weight)\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"### Testing\n***\nConverting the test data into sequences of integers and padding them.  \nLoading the best model and calculating the accuracy","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/int3405-sentiment-analysis-problem/test.csv')\ndata_test = pd.DataFrame({'input':df['Comment'],'id':df[\"RevId\"]})\nX_test = data_test['input'].values\n\ndef clean_text_test(X):\n    processed = []\n    for text in X:\n        text = list(tf.keras.preprocessing.text.text_to_word_sequence(str(text)))\n        text = \" \".join(text)\n        input_text_pre_no_accent = str(ViUtils.remove_accents(text).decode(\"utf-8\"))\n        input_text_pre_accent = ViTokenizer.tokenize(text)\n        processed.append(input_text_pre_accent)\n    return processed\n\nX_test_final = clean_text_test(X_test)\n\ntokenized_word_list = tokenizer.texts_to_sequences(X_test_final)\nX_test_padded = pad_sequences(tokenized_word_list, maxlen = maxlen, padding='post')\ny_pred = model.predict(X_test_padded)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:42:14.731767Z","iopub.execute_input":"2022-12-13T13:42:14.732457Z","iopub.status.idle":"2022-12-13T13:42:25.347291Z","shell.execute_reply.started":"2022-12-13T13:42:14.732072Z","shell.execute_reply":"2022-12-13T13:42:25.346361Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\nmodelload = load_model('sentiment_classifier.h5', custom_objects = {\"AttentionWithContext\" : AttentionWithContext, \"backend\" : backend})\ny_pred = modelload.predict(X_test_padded)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:42:25.349429Z","iopub.execute_input":"2022-12-13T13:42:25.349737Z","iopub.status.idle":"2022-12-13T13:42:30.948473Z","shell.execute_reply.started":"2022-12-13T13:42:25.349687Z","shell.execute_reply":"2022-12-13T13:42:30.947514Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"my_submission = pd.DataFrame({'RevId': np.array(df[\"RevId\"]).reshape(5103), 'Rating': np.array(y_pred).reshape(5103)})\nmy_submission.to_csv('submitTrain.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:43:20.334585Z","iopub.execute_input":"2022-12-13T13:43:20.334899Z","iopub.status.idle":"2022-12-13T13:43:20.463826Z","shell.execute_reply.started":"2022-12-13T13:43:20.334844Z","shell.execute_reply":"2022-12-13T13:43:20.463019Z"},"trusted":true},"execution_count":18,"outputs":[]}]}